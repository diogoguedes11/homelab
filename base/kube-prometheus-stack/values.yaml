# --- Global Settings ---
# Common labels applied to all resources created by this chart.
#commonLabels:

# --- CRD Management ---
# Configuration for Custom Resource Definitions managed by the stack.
crds:
  enabled: true
  # Security context for the CRD creation job pod.
  containerSecurityContext:
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    capabilities:
      drop:
      - ALL
  securityContext:
    fsGroup: 65534
    runAsGroup: 65534
    runAsNonRoot: true
    runAsUser: 65534
    seccompProfile:
      type: RuntimeDefault

# --- Prometheus Operator ---
# Configuration for the Prometheus Operator component.
prometheusOperator:
  tls:
    enabled: false
    tlsMinVersion: VersionTLS13
    internalPort: 10250
  admissionWebhooks:
    enabled: false
    failurePolicy: Ignore
    patch:
      enabled: false
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
        limits:
          cpu: 100m
          memory: 100Mi
      annotations:
        helm.sh/hook: post-install,post-upgrade
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
        seccompProfile:
          type: RuntimeDefault
    certManager:
      enabled: false
  replicas: 1
  resources:
    requests:
      cpu: 100m
      memory: 100Mi
    limits:
      cpu: 100m
      memory: 100Mi
  service:
    labels:
      k8s.observability.component: kube-prometheus-stack-operator
  serviceMonitor:
    enabled: true
    interval: 60s
    scrapeTimeout: 10s
    targetLabels:
    - cluster
    - environment
    - k8s.observability.component
    metricRelabelings:
    - sourceLabels: [ k8s_observability_component ]
      targetLabel: component
      regex: (.*)
      action: replace
    - regex: (k8s_observability_component)
      action: labeldrop
    additionalLabels:
      k8s.observability.servicemonitor: "true"

# --- Prometheus Instance ---
# Configuration for the Prometheus server instance(s).
prometheus:
  serviceAccount:
    create: true
  service:
    labels:
      k8s.observability.component: kube-prometheus-stack-prometheus
  serviceMonitor:
    selfMonitor: true
    enabled: true
    targetLabels:
    - cluster
    - environment
    - k8s.observability.component
  ingress:
    enabled: false
    ingressClassName: nginx
    annotations:
      nginx.ingress.kubernetes.io/rewrite-target: /$2
    paths:
    - /prometheus(/|$)(.*)
    pathType: ImplementationSpecific
  prometheusSpec:
    retention: 7d
    podMetadata:
      labels:
        k8s.observability.component: kube-prometheus-stack-prometheus
    scrapeInterval: 60s
    scrapeTimeout: 10s
    replicaExternalLabelName: "replica"
    prometheusExternalLabelName: "cluster"
    # remoteWrite:
    # - url: "http://mimir-distributed-nginx.observability.svc.cluster.local/api/v1/push"
    # queueConfig:
    #   capacity: 10000
    #   maxShards: 200
    #   minShards: 1
    podMonitorSelector:
      matchLabels:
        k8s.observability.podMonitor: "true"
    serviceMonitorSelector:
      matchLabels:
        k8s.observability.servicemonitor: "true"
    probeSelector:
      matchLabels:
        k8s.observability.probe: "true"
    ruleSelector:
      matchLabels:
        k8s.observability.rule: "true"
    enableFeatures:
    - auto-gomaxprocs
    securityContext:
      fsGroup: 65534
      runAsUser: 65534
    automountServiceAccountToken: null
    # alertingEndpoints:
    #   - namespace: observability
    #     name: kube-prometheus-stack-alertmanager-0
    #     port: http-web
    #     scheme: http
    #     pathPrefix: /
    #   - namespace: observability
    #     name: kube-prometheus-stack-alertmanager-1
    #     port: http-web
    #     scheme: http
    #     pathPrefix: /
    #   - namespace: observability
    #     name: kube-prometheus-stack-alertmanager-2
    #     port: http-web
    #     scheme: http
    #     pathPrefix: /
  resources:
    requests:
      cpu: 100m
      memory: 250Mi
    limits:
      cpu: 100m
      memory: 512Mi
    metricRelabelings:
    - sourceLabels: [ k8s_observability_component ]
      targetLabel: component
      regex: (.*)
      action: replace
    - regex: (k8s_observability_component)
      action: labeldrop

defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    configReloaders: true
    general: true
    k8sContainerCpuUsageSecondsTotal: true
    k8sContainerMemoryCache: true
    k8sContainerMemoryRss: true
    k8sContainerMemorySwap: true
    k8sContainerResource: true
    k8sContainerMemoryWorkingSetBytes: true
    k8sPodOwner: true
    kubeApiserverAvailability: true
    kubeApiserverBurnrate: true
    kubeApiserverHistogram: true
    kubeApiserverSlos: true
    kubeControllerManager: true
    kubelet: true
    kubeProxy: true
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeSchedulerAlerting: true
    kubeSchedulerRecording: true
    kubeStateMetrics: true
    network: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true
    windows: true

  ## Labels for default rules
  labels:
    k8s.observability.rule: "true"
  ## Annotations for default rules
  annotations: {}
# --- Alertmanager ---
# Configuration for the Alertmanager component.
alertmanager:
  enabled: true
  alertmanagerSpec:
    podMetadata:
      labels:
        k8s.observability.component: kube-prometheus-stack-alertmanager
    replicas: 2
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 3067
  service:
    labels:
      k8s.observability.component: kube-prometheus-stack-alertmanager
  serviceMonitor:
    targetLabels:
    - cluster
    - environment
    - k8s.observability.component
    selfMonitor: true
    interval: 60s
    additionalLabels:
      k8s.observability.servicemonitor: "true"

# --- Grafana ---
# Configuration for the Grafana visualization component.
grafana:
  enabled: true
  # admin:
  #   existingSecret: grafana-admin-secret
  #   userKey: admin-user
  #   passwordKey: admin-password
  # OIDC configuration sourced from an external secret
  # createConfigmap: false # Assuming Grafana config (like OIDC) is handled via envFromSecret
  # envFromSecret: grafana-oidc-secret
  serviceAccount:
    create: true
    autoMount: true
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      annotations:
        grafana_folder: "Kubernetes Core"
      folder: /tmp/dashboards
      folderAnnotation: grafana_folder
      provider:
        allowUiUpdates: true
        foldersFromFilesStructure: true
    resources:
      limits:
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi
  additionalDataSources:
    # - name: Tempo
    #   type: tempo
    #   uid: tempo
    #   access: proxy
    #   editable: true
    #   url: "http://tempo-distributed-query-frontend.observability.svc.cluster.local:3100"
    #   jsonData:
    #     tracesToLogsV2:
    #       datasourceUid: "loki"
    #       spanStartTimeShift: "-1h"
    #       spanEndTimeShift: "1h"
    #       filterByTraceID: true
    #       filterBySpanID: false
    #       tags: [ { key: "service.name", value: "service_name" } ]
    #     nodeGraph:
    #       enabled: true
    #     serviceMap:
    #       datasourceUid: "mimir"
    #     lokiSearch:
    #       datasourceUid: "loki"
    #     tracesToMetrics:
    #       datasourceUid: "mimir"
    #       spanStartTimeShift: "-30m"
    #       spanEndTimeShift: "30m"
    #       tags: [ { key: "service.name", value: "service_name" }, { key: "job" } ]
    #       queries:
    #       - name: "Sample query"
    #         query: "sum(rate(traces_span_metrics_duration_milliseconds_bucket{$$__tags}[5m]))"
    # - name: Mimir
    #   type: prometheus
    #   uid: mimir
    #   access: proxy
    #   editable: true
    #   url: "http://mimir-distributed-nginx.observability.svc.cluster.local/prometheus"
    # - name: Loki
    #   type: loki
    #   uid: loki
    #   access: proxy
    #   editable: true
    #   url: "http://loki-query-frontend.observability.svc.cluster.local:3100"
    #   basicAuth: false
    #   jsonData:
    #     maxLines: 1000
    #     derivedFields:
    #     - datasourceName: Tempo
    #       matcherType: "label"
    #       matcherRegex: trace[_]?id
    #       name: trace_id
    #       url: "$${__value.raw}"
    #       urlDisplayLabel: "View Trace"
    #       datasourceUid: tempo
  serviceMonitor:
    enabled: true
    targetLabels:
    - cluster
    - environment
    - k8s.observability.component
    labels:
      # Note: These labels might be misintended, usually interval/scrapeTimeout go directly under serviceMonitor
      interval: 60s
      scrapeTimeout: 10s
    metricRelabelings:
    - sourceLabels: [ k8s_observability_component ]
      targetLabel: component
      regex: (.*)
      action: replace
    - regex: (k8s_observability_component)
      action: labeldrop

# --- Kubernetes Service Monitors ---
# Global toggle for monitoring built-in Kubernetes components (if applicable in chart version)
kubernetesServiceMonitors:
  enabled: true # Assumed enabled based on individual components below being configured

# --- Control Plane Monitoring ---
# Configuration for monitoring Kubernetes control plane components.
kubeApiServer:
  enabled: false

kubeControllerManager:
  enabled: false

kubeScheduler:
  enabled: false

kubeEtcd:
  enabled: false

kubeProxy:
  enabled: false

# --- Kubelet Monitoring ---
# Configuration for monitoring Kubelet endpoints (including cAdvisor).
kubelet:
  # enabled: true # Assuming enabled as serviceMonitor is configured
  serviceMonitor:
    targetLabels:
    - cluster
    - environment
    - k8s.observability.component
    attachMetadata:
      node: true
    interval: 60s
    probes: true # Enable scraping /metrics/probes endpoint
    cAdvisor: true # Enable scraping /metrics/cadvisor endpoint
    cAdvisorInterval: 60s # Specific interval for cAdvisor, overrides global if needed
    additionalLabels:
      # Labels added to the ServiceMonitor object itself
      k8s.observability.servicemonitor: "true"
    # Relabelings for /metrics
    metricRelabelings:
    - sourceLabels: [ k8s_observability_component ]
      targetLabel: component
      regex: (.*)
      action: replace
    - regex: (k8s_observability_component)
      action: labeldrop

# --- Node Exporter ---
# Configuration for the node-exporter component (host metrics).
nodeExporter:
  # This usually controls the daemonset etc.
  enabled: true
  operatingSystems:
    # Specify which OS node-exporter daemonsets to enable
    linux:
      enabled: true
    aix:
      enabled: false
    darwin:
      enabled: false

prometheus-node-exporter:
  # This often refers to the embedded sub-chart configuration.~
  podLabels:
    jobLabel: node-exporter
    k8s.observability.component: node-exporter
  prometheus:
    monitor:
      # Configures the ServiceMonitor for node-exporter
      enabled: true
      interval: 60s
      targetLabels:
      - cluster
      - environment
      - k8s.observability.component
      scrapeTimeout: 10s
      attachMetadata:
        node: true # Add node labels to metrics
      metricRelabelings:
      - sourceLabels: [ k8s_observability_component ]
        targetLabel: component
        regex: (.*)
        action: replace
      - regex: (k8s_observability_component)
        action: labeldrop
  service:
    # Configure the node-exporter service
    labels:
      jobLabel: kube-prometheus-stack-node-exporter
      k8s.observability.component: kube-prometheus-stack-node-exporter

# --- Kube State Metrics ---
# Configuration for the kube-state-metrics component.
kubeStateMetrics:
  # This usually controls the deployment/daemonset/etc.
  enabled: true

kube-state-metrics:
  # This often refers to the embedded sub-chart configuration.
  prometheus:
    monitor:
      # Configures the ServiceMonitor for kube-state-metrics
      enabled: true
      interval: 60s
      scrapeTimeout: 10s
      targetLabels:
      - cluster
      - environment
      - k8s.observability.component
  service:
    enabled: true
    port: 8080

# --- CoreDNS Monitoring ---
# Configuration for monitoring CoreDNS.
coreDns:
  # enabled: true # Assuming enabled as serviceMonitor is configured
  # service.enabled: true # Assuming service exists for monitoring
  serviceMonitor:
    # enabled: true # Assuming enabled as interval is configured
    interval: 60s
    metricRelabelings:
    - sourceLabels: [ k8s_observability_component ]
      targetLabel: component
      regex: (.*)
      action: replace
    - regex: (k8s_observability_component)
      action: labeldrop
    additionalLabels:
      k8s.observability.servicemonitor: "true"

# additionalPrometheusRulesMap:
